algorithm.adv_estimator=grpo
 
data.train_files=${QUERIES_FILE}
 
data.val_files=${QUERIES_FILE}
 
+data.custom_cls.path=verl.utils.dataset.msmarco_dataset
 
+data.custom_cls.name=BeirRLDataset
 
data.train_batch_size=${BATCH_SIZE}
 
data.val_batch_size=16
 
data.max_prompt_length=${MAX_PROMPT_LEN}
 
data.max_response_length=${MAX_RESPONSE_LEN}
 
data.filter_overlong_prompts=True
 
data.truncation='error'
 
actor_rollout_ref.model.path=${MODEL_PATH}
 
actor_rollout_ref.model.use_remove_padding=True
 
actor_rollout_ref.model.enable_gradient_checkpointing=True
 
actor_rollout_ref.actor.optim.lr=${LEARNING_RATE}
 
actor_rollout_ref.actor.ppo_mini_batch_size=${BATCH_SIZE}
 
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8
 
actor_rollout_ref.actor.ppo_max_token_len_per_gpu=8000
 
actor_rollout_ref.actor.use_dynamic_bsz=True
 
actor_rollout_ref.actor.use_kl_loss=True
 
actor_rollout_ref.actor.kl_loss_coef=0.001
 
actor_rollout_ref.actor.kl_loss_type=low_var_kl
 
actor_rollout_ref.actor.entropy_coeff=0
 
actor_rollout_ref.actor.fsdp_config.param_offload=${OFFLOAD_PARAMS}
 
actor_rollout_ref.actor.fsdp_config.optimizer_offload=${OFFLOAD_OPTIMIZER}
 
actor_rollout_ref.actor.strategy=fsdp
 
actor_rollout_ref.rollout.name=vllm
 
actor_rollout_ref.rollout.gpu_memory_utilization=${GPU_MEMORY_UTIL}
 
actor_rollout_ref.rollout.tensor_model_parallel_size=1
 
actor_rollout_ref.rollout.n=${N_REWRITES}
 
actor_rollout_ref.rollout.temperature=0.7
 
actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=True
 
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16
 
actor_rollout_ref.rollout.val_kwargs.n=${N_REWRITES}
 
actor_rollout_ref.rollout.val_kwargs.do_sample=True
 
actor_rollout_ref.rollout.val_kwargs.temperature=0.7
 
actor_rollout_ref.ref.log_prob_use_dynamic_bsz=True
 
actor_rollout_ref.ref.fsdp_config.param_offload=True
 
actor_rollout_ref.ref.strategy=fsdp
 
critic.strategy=fsdp
 
algorithm.use_kl_in_reward=False
 
algorithm.norm_adv_by_std_in_grpo=False
 
reward_model.enable=False
 
reward_model.reward_manager=retrieval
 
+reward_model.faiss_index_path=${FAISS_INDEX}
 
+reward_model.id_mapping_path=${ID_MAPPING}
 
+reward_model.beir_dataset_path=${BEIR_DIR}
 
+reward_model.qrels_file=${QRELS_FILE}
 
+reward_model.quality_method=${QUALITY_METHOD}
 
+reward_model.k=${K}
 
+reward_model.embedding_model=${EMBEDDING_MODEL}
 
trainer.critic_warmup=0
 
trainer.logger='["console"]'
 
trainer.project_name=${PROJECT_NAME}
 
trainer.experiment_name=${EXPERIMENT_NAME}
 
trainer.n_gpus_per_node=1
 
trainer.nnodes=1
 
trainer.save_freq=50
 
trainer.test_freq=50
 
trainer.default_local_dir=${CHECKPOINT_DIR}
 
trainer.validation_data_dir=${CHECKPOINT_DIR}/rollouts
 
trainer.total_epochs=${EPOCHS}
 
$@